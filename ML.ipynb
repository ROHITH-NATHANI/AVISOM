{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOwG5ajM7/KKkJbvfen3QNP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ROHITH-NATHANI/AVISOM/blob/main/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data loading"
      ],
      "metadata": {
        "id": "3g0L-efUm1kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from  sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "uAi3SA28nEFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Raw Data\n",
        "path = \"/content/Lung Cancer Dataset.csv\"\n",
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "pv_fWtsCngGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "upwVV8TsnrH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "cQAtaC-7nuGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data exploration"
      ],
      "metadata": {
        "id": "5fScaLxxn1br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "VhQGj6Fkn8mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "KtZL_prXoBF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "6I0KT_CqoEJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "mLQc0UvxoHKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "DZEm6JCkoL0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['PULMONARY_DISEASE'].value_counts(normalize= True)"
      ],
      "metadata": {
        "id": "X14bNNSmoQnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data visualizations"
      ],
      "metadata": {
        "id": "w0FNlsRGoU-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot histograms\n",
        "df.hist(figsize=(12, 8), bins=20)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CarXYF-t5SoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j = df.copy()\n",
        "j['PULMONARY_DISEASE'] = pd.get_dummies(j['PULMONARY_DISEASE'],dtype=int,drop_first=True)\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.heatmap(j.corr(), annot=True, cmap=\"RdBu\",vmin=-1,)"
      ],
      "metadata": {
        "id": "NTW5H9CIogT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt # Import the matplotlib.pyplot module\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# Assuming your data is in \"Lung Cancer Dataset.csv\"\n",
        "df = pd.read_csv(\"/content/Lung Cancer Dataset.csv\") # Load the dataframe\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x=\"PULMONARY_DISEASE\", data=df)\n",
        "plt.title(\"Class Distribution before SMOTE\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qolEJ0kq5DPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(15, 5))\n",
        "sns.histplot(hue= df['SMOKING'],y =df['PULMONARY_DISEASE'],ax=axes[0][0])\n",
        "\n",
        "sns.histplot(hue= df['BREATHING_ISSUE'],y= df['PULMONARY_DISEASE'],ax=axes[0][1])\n",
        "\n",
        "sns.histplot(hue= df['THROAT_DISCOMFORT'],y =df['PULMONARY_DISEASE'],ax=axes[1][0])\n",
        "\n",
        "sns.histplot(hue= df['SMOKING_FAMILY_HISTORY'],y =df['PULMONARY_DISEASE'],ax=axes[1][1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TbV6iSRJopLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "sns.boxplot(y= df['AGE'],x =df['PULMONARY_DISEASE'],showfliers=True,ax=axes[0])\n",
        "\n",
        "sns.boxplot(y= df['ENERGY_LEVEL'],x =df['PULMONARY_DISEASE'],showfliers=True,ax=axes[1])\n",
        "\n",
        "sns.boxplot(y= df['OXYGEN_SATURATION'],x =df['PULMONARY_DISEASE'],showfliers=True,ax=axes[2])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JkVsgVCIovoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data preprocessing"
      ],
      "metadata": {
        "id": "7xuBemsDo1t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"Lung Cancer Dataset.csv\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values:\\n\", missing_values)\n",
        "\n",
        "# Fill or drop missing values (example: filling with median)\n",
        "df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "\n",
        "# Save preprocessed dataset\n",
        "df.to_csv(\"preprocessed_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "hOk-JVXl5h0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode categorical target variable\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"PULMONARY_DISEASE\"] = label_encoder.fit_transform(df[\"PULMONARY_DISEASE\"])\n",
        "\n",
        "# Save encoded dataset\n",
        "df.to_csv(\"encoded_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "v4hFzwjf5rpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Save scaled dataset\n",
        "df.to_csv(\"scaled_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "C4sLH6xX6C-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Remove low-variance features\n",
        "selector = VarianceThreshold(threshold=0.01)  # Adjust threshold as needed\n",
        "df_selected = pd.DataFrame(selector.fit_transform(df), columns=df.columns[selector.get_support()])\n",
        "\n",
        "# Save feature-selected dataset\n",
        "df_selected.to_csv(\"selected_features_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "GtFV-67O6GxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import zscore\n",
        "\n",
        "# Compute Z-scores\n",
        "z_scores = df[numeric_cols].apply(zscore)\n",
        "\n",
        "# Keep only rows where Z-score is within acceptable range (-3 to 3)\n",
        "df_cleaned = df[(z_scores.abs() < 3).all(axis=1)]\n",
        "\n",
        "# Save cleaned dataset\n",
        "df_cleaned.to_csv(\"outliers_removed_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "CE1JL-2M6NNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def processing(d):\n",
        "\n",
        "  df_processed = d.copy()\n",
        "\n",
        "  # Data Preprocessing :\n",
        "\n",
        "  #target :\n",
        "  df_processed['PULMONARY_DISEASE'] = pd.get_dummies(df_processed['PULMONARY_DISEASE'],dtype=int,drop_first=True)\n",
        "\n",
        "  #normalisation :\n",
        "  scaler = MinMaxScaler()\n",
        "  data = df_processed[['AGE','ENERGY_LEVEL','OXYGEN_SATURATION']]\n",
        "  scaler.fit(data)\n",
        "  df_processed[['AGE','ENERGY_LEVEL','OXYGEN_SATURATION']] = scaler.transform(data)\n",
        "  df_processed[['AGE','ENERGY_LEVEL','OXYGEN_SATURATION']]\n",
        "\n",
        "  return df_processed"
      ],
      "metadata": {
        "id": "Vu0v9Cr9pAW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def processing(d):\n",
        "\n",
        "  df_processed = d.copy()\n",
        "\n",
        "  # Data Preprocessing :\n",
        "\n",
        "  #target :\n",
        "  df_processed['PULMONARY_DISEASE'] = pd.get_dummies(df_processed['PULMONARY_DISEASE'],dtype=int,drop_first=True)\n",
        "\n",
        "  #normalisation :\n",
        "  # Import MinMaxScaler here\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  scaler = MinMaxScaler()\n",
        "  data = df_processed[['AGE','ENERGY_LEVEL','OXYGEN_SATURATION']]\n",
        "  scaler.fit(data)\n",
        "  df_processed[['AGE','ENERGY_LEVEL','OXYGEN_SATURATION']] = scaler.transform(data)\n",
        "  df_processed[['AGE','ENERGY_LEVEL','OXYGEN_SATURATION']]\n",
        "\n",
        "  return df_processed"
      ],
      "metadata": {
        "id": "cvYM6ACgpQFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g= sns.pairplot(df,vars=['AGE','ENERGY_LEVEL','OXYGEN_SATURATION'],hue='PULMONARY_DISEASE') # Replace 'df_full' with 'df' or another appropriate DataFrame\n",
        "g.map_lower(sns.kdeplot, fill=True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jtOKoskYpULL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/Lung Cancer Dataset.csv\")\n",
        "\n",
        "# Display the first few rows and info\n",
        "print(\"Data Head:\")\n",
        "print(df.head())\n",
        "print(\"\\nData Info:\")\n",
        "print(df.info())\n"
      ],
      "metadata": {
        "id": "16hQzLS37sIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ENCODING Categorical Target Variable\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a LabelEncoder instance\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Encode the target variable \"PULMONARY_DISEASE\" (e.g., YES -> 1, NO -> 0)\n",
        "df[\"PULMONARY_DISEASE\"] = le.fit_transform(df[\"PULMONARY_DISEASE\"])\n",
        "\n",
        "# Verify encoding by printing unique values\n",
        "print(\"Encoded Target Values:\", df[\"PULMONARY_DISEASE\"].unique())\n"
      ],
      "metadata": {
        "id": "xR_LPxt3700j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data normalization"
      ],
      "metadata": {
        "id": "XJhIM9j5pe9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/Lung Cancer Dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Data Normalization\n",
        "scaler = MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(df.drop(columns=[\"PULMONARY_DISEASE\"]))\n",
        "\n",
        "# Convert back to DataFrame\n",
        "normalized_df = pd.DataFrame(normalized_data, columns=df.drop(columns=[\"PULMONARY_DISEASE\"]).columns)\n",
        "\n",
        "# Add target variable back\n",
        "normalized_df[\"PULMONARY_DISEASE\"] = df[\"PULMONARY_DISEASE\"].values\n",
        "\n",
        "# Save the normalized data\n",
        "normalized_df.to_csv(\"/content/Lung Cancer Dataset.csv\", index=False)\n",
        "\n",
        "print(\"Data normalization complete. Saved as 'Normalized_Lung_Cancer_Dataset.csv'\")\n"
      ],
      "metadata": {
        "id": "izE8yYul4ZGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/Lung Cancer Dataset.csv\")\n",
        "\n",
        "# Separate features and target variable\n",
        "features = df.drop(columns=[\"PULMONARY_DISEASE\"])\n",
        "target = df[\"PULMONARY_DISEASE\"]\n",
        "\n",
        "# Normalize features using RobustScaler\n",
        "scaler = RobustScaler()\n",
        "normalized_features = scaler.fit_transform(features)\n",
        "\n",
        "# Convert the normalized features back to a DataFrame\n",
        "normalized_df = pd.DataFrame(normalized_features, columns=features.columns)\n",
        "\n",
        "# Add the target variable back to the DataFrame\n",
        "normalized_df[\"PULMONARY_DISEASE\"] = target.values\n",
        "\n",
        "# Save the normalized data to a new CSV file\n",
        "normalized_df.to_csv(\"/content/Lung Cancer Dataset.csv\", index=False)\n",
        "\n",
        "print(\"Data normalization complete using RobustScaler. Saved as 'Normalized_Lung_Cancer_Dataset_Robust.csv'\")\n"
      ],
      "metadata": {
        "id": "QqpBNsFC6Tbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize Data Using MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Separate features (all columns except target) and target\n",
        "features = df.drop(columns=[\"PULMONARY_DISEASE\"])\n",
        "target = df[\"PULMONARY_DISEASE\"]\n",
        "\n",
        "# Initialize MinMaxScaler and apply to features\n",
        "scaler = MinMaxScaler()\n",
        "normalized_features = scaler.fit_transform(features)\n",
        "\n",
        "# Convert the normalized array back to a DataFrame\n",
        "normalized_df = pd.DataFrame(normalized_features, columns=features.columns)\n",
        "\n",
        "# Add the target column back into the DataFrame\n",
        "normalized_df[\"PULMONARY_DISEASE\"] = target.values\n",
        "\n",
        "# Preview the normalized data\n",
        "print(\"Normalized Data using MinMaxScaler:\")\n",
        "print(normalized_df.head())\n"
      ],
      "metadata": {
        "id": "g3vGMEyU8Gp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Normalize Data Using RobustScaler\n",
        " from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Separate features and target again\n",
        "features = df.drop(columns=[\"PULMONARY_DISEASE\"])\n",
        "target = df[\"PULMONARY_DISEASE\"]\n",
        "\n",
        "# Initialize RobustScaler and apply to features\n",
        "scaler = RobustScaler()\n",
        "robust_features = scaler.fit_transform(features)\n",
        "\n",
        "# Convert the scaled data back into a DataFrame\n",
        "robust_df = pd.DataFrame(robust_features, columns=features.columns)\n",
        "\n",
        "# Reattach the target variable\n",
        "robust_df[\"PULMONARY_DISEASE\"] = target.values\n",
        "\n",
        "# Preview the robust-scaled data\n",
        "print(\"Normalized Data using RobustScaler:\")\n",
        "print(robust_df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "Ta1zhPZW8Rnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# data splitting"
      ],
      "metadata": {
        "id": "zZEZiTCQ5ESh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop(columns=[\"PULMONARY_DISEASE\"])\n",
        "y = df[\"PULMONARY_DISEASE\"]\n",
        "\n",
        "# Split the dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save train and test sets\n",
        "X_train.to_csv(\"X_train.csv\", index=False)\n",
        "X_test.to_csv(\"X_test.csv\", index=False)\n",
        "y_train.to_csv(\"y_train.csv\", index=False)\n",
        "y_test.to_csv(\"y_test.csv\", index=False)"
      ],
      "metadata": {
        "id": "6czK9oa86dxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd  # Import pandas\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit LabelEncoder on y_train and transform\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Convert y_train_balanced to a DataFrame before saving to CSV\n",
        "y_train_balanced_df = pd.DataFrame(y_train_balanced, columns=['PULMONARY_DISEASE']) # Convert to DataFrame\n",
        "\n",
        "# Save the balanced dataset\n",
        "X_train_balanced.to_csv(\"X_train_balanced.csv\", index=False)\n",
        "y_train_balanced_df.to_csv(\"y_train_balanced.csv\", index=False) # Save the DataFrame\n",
        "\n",
        "print(\"SMOTE applied successfully!\")"
      ],
      "metadata": {
        "id": "oZMLsGIi6nc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Apply PCA (keeping 95% variance)\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_pca = pca.fit_transform(X_train_balanced)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "# Save PCA-transformed dataset\n",
        "pd.DataFrame(X_train_pca).to_csv(\"X_train_pca.csv\", index=False)\n",
        "pd.DataFrame(X_test_pca).to_csv(\"X_test_pca.csv\", index=False)"
      ],
      "metadata": {
        "id": "utFpXA0L66Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load normalized dataset\n",
        "df = pd.read_csv(\"/content/Lung Cancer Dataset.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"PULMONARY_DISEASE\"])\n",
        "y = df[\"PULMONARY_DISEASE\"]\n",
        "\n",
        "# Basic train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Basic Split:\")\n",
        "print(\"Training features shape:\", X_train.shape)\n",
        "print(\"Test features shape:\", X_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "28kO1z5h5MRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load normalized dataset\n",
        "df = pd.read_csv(\"/content/Lung Cancer Dataset.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"PULMONARY_DISEASE\"])\n",
        "y = df[\"PULMONARY_DISEASE\"]\n",
        "\n",
        "# Stratified train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Stratified Split:\")\n",
        "print(\"Training features shape:\", X_train.shape)\n",
        "print(\"Test features shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "pFgwquX07GtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Load normalized dataset\n",
        "df = pd.read_csv(\"/content/Lung Cancer Dataset.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"PULMONARY_DISEASE\"])\n",
        "y = df[\"PULMONARY_DISEASE\"]\n",
        "\n",
        "# KFold splitting (5 folds)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    print(f\"Fold {fold + 1}:\")\n",
        "    print(\"  Training features shape:\", X_train.shape)\n",
        "    print(\"  Test features shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "MheYTYh-7PgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# Load normalized dataset\n",
        "df = pd.read_csv(\"/content/Lung Cancer Dataset.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"PULMONARY_DISEASE\"])\n",
        "y = df[\"PULMONARY_DISEASE\"]\n",
        "\n",
        "# Define ShuffleSplit parameters (1 split, 80% train, 20% test)\n",
        "ss = ShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "\n",
        "for train_index, test_index in ss.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "print(\"ShuffleSplit:\")\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Test set shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "D5xwcSih7Wdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature selection"
      ],
      "metadata": {
        "id": "yk4V5rp-7Up8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"SMOKING_DISCOLORATION\"] = df[\"SMOKING\"] * df[\"FINGER_DISCOLORATION\"]\n",
        "\n",
        "# Save dataset with new features\n",
        "df.to_csv(\"feature_engineered_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "xWiZbJvn7i6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Compute Mutual Information scores\n",
        "mi_scores = mutual_info_classif(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "mi_scores_df = pd.DataFrame({\"Feature\": X_train_balanced.columns, \"MI Score\": mi_scores})\n",
        "mi_scores_df = mi_scores_df.sort_values(by=\"MI Score\", ascending=False)\n",
        "\n",
        "# Select top features (e.g., top 10)\n",
        "top_features = mi_scores_df[\"Feature\"].head(10).tolist()\n",
        "X_train_selected = X_train_balanced[top_features]\n",
        "X_test_selected = X_test[top_features]\n",
        "\n",
        "# Save feature-selected datasets\n",
        "X_train_selected.to_csv(\"X_train_selected.csv\", index=False)\n",
        "X_test_selected.to_csv(\"X_test_selected.csv\", index=False)\n",
        "\n",
        "print(\"Feature selection completed!\")\n"
      ],
      "metadata": {
        "id": "taRr6go47vd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = X_train_balanced.corr().abs()\n",
        "\n",
        "# Find highly correlated features\n",
        "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "high_corr_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
        "\n",
        "# Drop highly correlated features\n",
        "X_train_uncorrelated = X_train_balanced.drop(columns=high_corr_features)\n",
        "X_test_uncorrelated = X_test.drop(columns=high_corr_features)\n",
        "\n",
        "# Save the datasets\n",
        "X_train_uncorrelated.to_csv(\"X_train_uncorrelated.csv\", index=False)\n",
        "X_test_uncorrelated.to_csv(\"X_test_uncorrelated.csv\", index=False)\n",
        "\n",
        "print(\"Highly correlated features removed!\")\n"
      ],
      "metadata": {
        "id": "Dn_OESFZ70up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define noise function\n",
        "def add_noise(data, noise_level=0.02):\n",
        "    noise = np.random.normal(0, noise_level, data.shape)\n",
        "    return data + noise\n",
        "\n",
        "# Apply noise to numerical features\n",
        "X_train_augmented = X_train_balanced.copy()\n",
        "X_train_augmented += add_noise(X_train_augmented, noise_level=0.01)\n",
        "\n",
        "# Save augmented dataset\n",
        "X_train_augmented.to_csv(\"X_train_augmented.csv\", index=False)\n",
        "\n",
        "print(\"Data augmentation completed!\")"
      ],
      "metadata": {
        "id": "QV3tbCnD75Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load final dataset\n",
        "\n",
        "\n",
        "# Display basic statistics\n",
        "print(df.describe())\n",
        "\n",
        "# Display class distribution\n",
        "print(df[\"PULMONARY_DISEASE\"].value_counts())\n"
      ],
      "metadata": {
        "id": "YXBYcXON7903"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data tranning"
      ],
      "metadata": {
        "id": "Ec1GQFVQ8dAY"
      }
    },
    {
      "source": [
        "!pip install xgboost lightgbm imbalanced-learn\n",
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier,\n",
        "    BaggingClassifier\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"/content/Lung Cancer Dataset.csv\")\n",
        "X = df.drop(columns=[\"PULMONARY_DISEASE\"])\n",
        "y = df[\"PULMONARY_DISEASE\"]\n",
        "\n",
        "# ----> Convert object type columns to numerical using Label Encoding if needed <----\n",
        "for column in X.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X[column] = le.fit_transform(X[column])\n",
        "\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Encode target variable\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"LightGBM\": LGBMClassifier(),\n",
        "    \"Extra Trees\": ExtraTreesClassifier(),\n",
        "    \"Bagging\": BaggingClassifier(),\n",
        "    \"LDA\": LinearDiscriminantAnalysis(),\n",
        "}\n",
        "\n",
        "# Hyperparameter grids\n",
        "param_grids = {\n",
        "    \"Logistic Regression\": {\"C\": [0.001, 0.01, 0.1, 1, 10]},\n",
        "    \"Random Forest\": {\"n_estimators\": [50, 100, 200], \"max_depth\": [None, 10, 20, 30], \"min_samples_split\": [2, 5, 10]},\n",
        "    \"XGBoost\": {\"n_estimators\": [50, 100, 200], \"learning_rate\": [0.01, 0.1, 0.2], \"max_depth\": [3, 5, 10]},\n",
        "    \"Gradient Boosting\": {\"n_estimators\": [50, 100, 200], \"learning_rate\": [0.01, 0.1, 0.2], \"max_depth\": [3, 5, 10]},\n",
        "    \"LightGBM\": {\"n_estimators\": [50, 100, 200], \"learning_rate\": [0.01, 0.1, 0.2], \"num_leaves\": [20, 31, 40]},\n",
        "    \"Extra Trees\": {\"n_estimators\": [50, 100, 200], \"max_depth\": [None, 10, 20, 30], \"min_samples_split\": [2, 5, 10]},\n",
        "    \"Bagging\": {\"n_estimators\": [10, 50, 100]},\n",
        "    \"LDA\": {},\n",
        "}\n",
        "\n",
        "# Dictionary to store best models\n",
        "best_models = {}\n",
        "\n",
        "# Train models with best hyperparameters\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    if name in param_grids and param_grids[name]:  # If hyperparameters exist\n",
        "        grid_search = RandomizedSearchCV(\n",
        "            model, param_grids[name], n_iter=10, cv=3, scoring=\"accuracy\", n_jobs=-1, random_state=42\n",
        "        )\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_models[name] = grid_search.best_estimator_\n",
        "        print(f\"Best params for {name}: {grid_search.best_params_}\")\n",
        "\n",
        "    else:  # No tuning needed (e.g., LDA)\n",
        "        model.fit(X_train, y_train)\n",
        "        best_models[name] = model\n",
        "\n",
        "# Evaluate tuned models\n",
        "results = {}\n",
        "for name, model in best_models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results[name] = acc\n",
        "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Remove models with accuracy below 90%\n",
        "filtered_results = {k: v for k, v in results.items() if v >= 0.90}\n",
        "\n",
        "if filtered_results:\n",
        "    best_model_name = max(filtered_results, key=filtered_results.get)\n",
        "    print(f\"\\nBest Model After Tuning (Above 90%): {best_model_name} with Accuracy: {filtered_results[best_model_name]:.4f}\")\n",
        "else:\n",
        "    print(\"\\nNo model achieved accuracy above 90%.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ZBP22YnNjNx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/Lung Cancer Dataset.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"PULMONARY_DISEASE\"])\n",
        "y = df[\"PULMONARY_DISEASE\"]\n",
        "\n",
        "# ----> Convert object type columns to numerical using Label Encoding if needed <----\n",
        "for column in X.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X[column] = le.fit_transform(X[column])\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a LabelEncoder instance\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to your training target variable and transform it\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "}\n",
        "\n",
        "# Create and train the XGBoost model with hyperparameter tuning\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "random_search = RandomizedSearchCV(\n",
        "    model, param_distributions=param_grid, n_iter=10, cv=3, scoring='accuracy', random_state=42, n_jobs=-1\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and make predictions\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "92lXOgzSNenR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier,\n",
        "    BaggingClassifier\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from xgboost import XGBClassifier # Already imported\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/Lung Cancer Dataset.csv\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"PULMONARY_DISEASE\"])\n",
        "y = df[\"PULMONARY_DISEASE\"]\n",
        "\n",
        "# ----> Convert object type columns to numerical using Label Encoding if needed <----\n",
        "for column in X.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X[column] = le.fit_transform(X[column])\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a LabelEncoder instance\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to your training target variable and transform it\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "# Define the parameter grid for RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "}\n",
        "\n",
        "\n",
        "# Define models and hyperparameter grids (moved from previous cell)\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"LightGBM\": LGBMClassifier(),\n",
        "    \"Extra Trees\": ExtraTreesClassifier(),\n",
        "    \"Bagging\": BaggingClassifier(),\n",
        "    \"LDA\": LinearDiscriminantAnalysis(),\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    \"Logistic Regression\": {\"C\": [0.001, 0.01, 0.1, 1, 10]},\n",
        "    \"Random Forest\": {\"n_estimators\": [50, 100, 200], \"max_depth\": [None, 10, 20, 30], \"min_samples_split\": [2, 5, 10]},\n",
        "    \"XGBoost\": {\"n_estimators\": [50, 100, 200], \"learning_rate\": [0.01, 0.1, 0.2], \"max_depth\": [3, 5, 10]},\n",
        "    \"Gradient Boosting\": {\"n_estimators\": [50, 100, 200], \"learning_rate\": [0.01, 0.1, 0.2], \"max_depth\": [3, 5, 10]},\n",
        "    \"LightGBM\": {\"n_estimators\": [50, 100, 200], \"learning_rate\": [0.01, 0.1, 0.2], \"num_leaves\": [20, 31, 40]},\n",
        "    \"Extra Trees\": {\"n_estimators\": [50, 100, 200], \"max_depth\": [None, 10, 20, 30], \"min_samples_split\": [2, 5, 10]},\n",
        "    \"Bagging\": {\"n_estimators\": [10, 50, 100]},\n",
        "    \"LDA\": {},\n",
        "}\n",
        "\n",
        "# Dictionary to store best models\n",
        "best_models = {}\n",
        "\n",
        "# Train models with best hyperparameters (moved from previous cell)\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning {name}...\")\n",
        "\n",
        "    if name in param_grids and param_grids[name]:  # If hyperparameters exist\n",
        "        grid_search = RandomizedSearchCV(\n",
        "            model, param_grids[name], n_iter=10, cv=3, scoring=\"accuracy\", n_jobs=-1, random_state=42\n",
        "        )\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        best_models[name] = grid_search.best_estimator_\n",
        "        print(f\"Best params for {name}: {grid_search.best_params_}\")\n",
        "\n",
        "    else:  # No tuning needed (e.g., LDA)\n",
        "        model.fit(X_train, y_train)\n",
        "        best_models[name] = model\n",
        "\n",
        "# Now you can evaluate with confusion matrix\n",
        "for name, model in best_models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Print accuracy and classification report\n",
        "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Generate and display confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(f\"Confusion Matrix for {name}\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "Qode5DkMOCOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/Lung Cancer Dataset.csv\")\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop(columns=[\"PULMONARY_DISEASE\"])\n",
        "y = df[\"PULMONARY_DISEASE\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define individual models with parameter grids for tuning\n",
        "model1 = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "param_grid1 = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "}\n",
        "\n",
        "model2 = RandomForestClassifier()\n",
        "param_grid2 = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "\n",
        "# Create RandomizedSearchCV objects for each model\n",
        "random_search1 = RandomizedSearchCV(model1, param_distributions=param_grid1, n_iter=10, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "random_search2 = RandomizedSearchCV(model2, param_distributions=param_grid2, n_iter=10, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "\n",
        "# Create a VotingClassifier with the tuned models using soft voting\n",
        "hybrid_model = VotingClassifier(estimators=[('xgb', random_search1), ('rf', random_search2)], voting='soft')\n",
        "\n",
        "# Train the hybrid model (this will also tune the individual models)\n",
        "hybrid_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_train_pred = hybrid_model.predict(X_train)\n",
        "\n",
        "# Calculate evaluation metrics for training set\n",
        "training_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "training_precision = precision_score(y_train, y_train_pred, average='binary', pos_label='YES')\n",
        "training_recall = recall_score(y_train, y_train_pred, average='binary', pos_label='YES')\n",
        "training_f1_score = f1_score(y_train, y_train_pred, average='binary', pos_label='YES')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Training Accuracy:\", training_accuracy)\n",
        "print(\"Training Precision:\", training_precision)\n",
        "print(\"Training Recall:\", training_recall)\n",
        "print(\"Training F1-score:\", training_f1_score)"
      ],
      "metadata": {
        "id": "k2JSVxckc_5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/Lung Cancer Dataset.csv\")\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df.drop(columns=[\"PULMONARY_DISEASE\"])\n",
        "y = df[\"PULMONARY_DISEASE\"]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define individual models with parameter grids for tuning\n",
        "model1 = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "param_grid1 = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "}\n",
        "\n",
        "model2 = RandomForestClassifier()\n",
        "param_grid2 = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "\n",
        "# Create RandomizedSearchCV objects for each model\n",
        "random_search1 = RandomizedSearchCV(model1, param_distributions=param_grid1, n_iter=10, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "random_search2 = RandomizedSearchCV(model2, param_distributions=param_grid2, n_iter=10, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
        "\n",
        "# Create a VotingClassifier with the tuned models using soft voting\n",
        "hybrid_model = VotingClassifier(estimators=[('xgb', random_search1), ('rf', random_search2)], voting='soft')\n",
        "\n",
        "# Train the hybrid model (this will also tune the individual models)\n",
        "hybrid_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the training set\n",
        "y_train_pred = hybrid_model.predict(X_train)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_train, y_train_pred)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['NO', 'YES'], yticklabels=['NO', 'YES'])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix of Soft Voting Hybrid Model\")\n",
        "plt.show()\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "h4wFCVWEHRXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Convert categorical labels ('YES', 'NO') to binary (1, 0)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)  # Converts 'YES' -> 1, 'NO' -> 0\n",
        "y_test = label_encoder.transform(y_test)  # Ensures consistent encoding"
      ],
      "metadata": {
        "id": "-iGWKCz68DnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load dataset (Ensure X and y are preprocessed)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert categorical target labels ('YES', 'NO') to binary (1, 0)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "\n",
        "# Define individual models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "    \"LightGBM\": LGBMClassifier(),\n",
        "    \"Extra Trees\": ExtraTreesClassifier(),\n",
        "    \"Bagging\": BaggingClassifier(),\n",
        "    \"LDA\": LinearDiscriminantAnalysis(),\n",
        "}\n",
        "\n",
        "# Define Hybrid Models\n",
        "soft_voting_model = VotingClassifier(estimators=[\n",
        "    (\"XGBoost\", XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
        "    (\"Random Forest\", RandomForestClassifier())\n",
        "], voting='soft')\n",
        "\n",
        "tuned_xgboost = XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.05, use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Add hybrid models to dictionary\n",
        "models[\"Soft Voting Hybrid Model\"] = soft_voting_model\n",
        "models[\"Tuned XGBoost\"] = tuned_xgboost\n",
        "\n",
        "# Plot ROC Curve for each model\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_probs = model.predict_proba(X_test)[:, 1]  # Get probability for class 1 (positive case)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_probs, pos_label=1)  # Explicitly define pos_label\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "# Plot baseline (random guess)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random (AUC = 0.50)\")\n",
        "\n",
        "# Customize plot\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC-AUC Curve for Model Comparison (Including Hybrid Models)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4eA9igct8RY6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}